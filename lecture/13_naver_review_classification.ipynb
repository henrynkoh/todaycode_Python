{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"256px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"13_naver_review_classification.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5jJByOL8LD0p","colab_type":"text"},"source":["[e9t/nsmc: Naver sentiment movie corpus](https://github.com/e9t/nsmc)"]},{"cell_type":"code","metadata":{"id":"hCY8YK1iLD0q","colab_type":"code","colab":{}},"source":["# 라이브러리로드하고 폰트 설정하기\n","# pandas, numpy, seaborn\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jogIJDg1LD0u","colab_type":"text"},"source":["## 데이터 로드하기"]},{"cell_type":"code","metadata":{"id":"nmsznUQFLD0u","colab_type":"code","colab":{}},"source":["# train\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pN6wLNgALD0x","colab_type":"code","colab":{}},"source":["# test\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"jZ7VP5dFLD00","colab_type":"code","colab":{}},"source":["# head 로 미리보기\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoolpzFHLD02","colab_type":"code","colab":{}},"source":["# tail로 미리보기\n","test.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9cxGDqQLD04","colab_type":"code","colab":{}},"source":["# info로 정보보기\n","train.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJvBLvTuLD07","colab_type":"text"},"source":["### 중복 데이터 제거"]},{"cell_type":"code","metadata":{"id":"EX6NEL3RLD08","colab_type":"code","colab":{}},"source":["# drop_duplicates 로 train 중복제거\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQvyW_s-LD0-","colab_type":"code","colab":{}},"source":["# drop_duplicates 로 test 중복제거\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4Gj8b2ILD1B","colab_type":"text"},"source":["### 결측치 제거"]},{"cell_type":"code","metadata":{"id":"T1mPI8yPLD1B","colab_type":"code","colab":{}},"source":["# train\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_hhA4B-LD1D","colab_type":"code","colab":{}},"source":["# test\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3P5o42XQLD1G","colab_type":"text"},"source":["### 빠른 학습을 위해 일부만 샘플링"]},{"cell_type":"code","metadata":{"id":"Cs_MXSGlLD1G","colab_type":"code","colab":{}},"source":["# train\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JdoWw0sLD1J","colab_type":"code","colab":{}},"source":["# test\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5i63f23LD1M","colab_type":"text"},"source":["### 결측치 확인"]},{"cell_type":"code","metadata":{"id":"2ECt-XanLD1M","colab_type":"code","colab":{}},"source":["#  train\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsvQW6o1LD1O","colab_type":"code","colab":{}},"source":["# test\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DZDNPDZLD1Q","colab_type":"text"},"source":["## 정답값 보기"]},{"cell_type":"code","metadata":{"id":"UJeekGdfLD1R","colab_type":"code","colab":{}},"source":["# value_counts 로 빈도수 세기\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRF9h2UfLD1T","colab_type":"code","colab":{}},"source":["# value_counts 로 비율 보기\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"El27V4e1LD1W","colab_type":"code","colab":{}},"source":["# countplot 으로 빈도수 세기\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itCm-QXaLD1Y","colab_type":"text"},"source":["## 문서 분석하기"]},{"cell_type":"markdown","metadata":{"id":"xHF1iicuLD1Y","colab_type":"text"},"source":["### 중복값 보기"]},{"cell_type":"code","metadata":{"id":"c7ulgFWNLD1Z","colab_type":"code","colab":{}},"source":["# nunique 값 보기\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"THtI7yo2LD1b","colab_type":"code","colab":{}},"source":["# 중복데이터 확인\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_sidoz88LD1d","colab_type":"text"},"source":["### 단어 수 세기"]},{"cell_type":"code","metadata":{"id":"Yh89qCrhLD1e","colab_type":"code","colab":{}},"source":["# len\n","# word_count - split 을 통해 리스트로 만들고 갯수를 셉니다.\n","# unique_word_count - split을 통해 리스트로 만든 값에서 set 을 해주면 중복을 제거합니다.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLgavqRaLD1f","colab_type":"code","colab":{}},"source":["# \"len\", \"word_count\", \"unique_word_count\" 을 봅니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGtM4tYDLD1h","colab_type":"code","colab":{}},"source":["# \"len\", \"word_count\", \"unique_word_count\" 을 distplot으로 시각화 합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlMN_R0-LD1j","colab_type":"code","colab":{}},"source":["import re\n","\n","def preprocessing(text):\n","    # 특수문자 제거\n","    # 특수문자나 이모티콘 등은 때로는 의미를 갖기도 하지만 여기에서는 제거했습니다.\n","    # text = re.sub('[?.,;:|\\)*~`’!^\\-_+<>@\\#$%&-=#}※]', '', text)\n","    # 한글, 영문, 숫자만 남기고 모두 제거하도록 합니다.\n","    # text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', ' ', text)\n","    # 한글, 영문만 남기고 모두 제거하도록 합니다.\n","    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', text)\n","    # 중복으로 생성된 공백값을 제거합니다.\n","    text = re.sub(' +', ' ', text)\n","    # 중복으로 생성된 ㅋ 값을 제거합니다.\n","    #\n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkbbqUmKLD1n","colab_type":"code","colab":{}},"source":["# 아래의 결과가  \"재미있었어요ㅋ\" 가 나오도록 위 preprocessing 을 고쳐보세요.\n","preprocessing(\"재미있었어요ㅋㅋㅋㅋㅋ\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCY0mEAOLD1p","colab_type":"code","colab":{}},"source":["# preprocessing 함수를 적용합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bUtrs0L9LD1r","colab_type":"text"},"source":["### 불용어 제거"]},{"cell_type":"code","metadata":{"id":"KHHXtRIKLD1r","colab_type":"code","colab":{}},"source":["# 불용어 제거\n","def remove_stopwords(text):\n","    tokens = text.split(' ')\n","    stops = ['안녕', '안녕하세요', '있습니다', '하는', \n","             '그', '및', '제', '할', '하고', '더', '대한', \n","             '한', '그리고', '월', '저는', '없는', '입니다', \n","             '등', '일', '많은', '이런', '것은', '왜','같은', \n","             '같습니다', '없습니다', '위해', '한다']\n","    meaningful_words = [w for w in tokens if not w in stops]\n","    return ' '.join(meaningful_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7mVXECCLD1t","colab_type":"code","colab":{}},"source":["# remove_stopwords 함수를 적용합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zd3rq2fFLD1u","colab_type":"text"},"source":["### 워드클라우드로 빈도수 표현하기\n","[amueller/word_cloud: A little word cloud generator in Python](https://github.com/amueller/word_cloud)\n","\n","* 별도의 설치가 필요합니다. \n","* 다음 명령어로 설치가 가능합니다. conda prompt 혹은 터미널을 열어 설치해 주세요.\n","\n","* conda 사용시 : `conda install -c conda-forge wordcloud`\n","* pip 사용시 : `pip install wordcloud`"]},{"cell_type":"code","metadata":{"id":"TXgyOPUQLD1v","colab_type":"code","colab":{}},"source":["# 공식문서의 튜토리얼을 보고 wordcloud를 그리는 함수를 만들어 봅니다.\n","# 이때 폰트 설정시 폰트명이 아닌 폰트의 설치 경로를 입력해 주셔야 합니다.\n","# 윈도우 :  r\"C:\\Windows\\Fonts\\Malgun Gothic.ttf\" 해당 경로에 폰트가 있는지 확인을 해주세요.\n","# 맥 : r\"/Library/Fonts/AppleGothic.ttf\"\n","# 나눔고딕 등의 폰트를 설치했다면 : '/Library/Fonts/NanumBarunGothic.ttf'\n","\n","from wordcloud import WordCloud\n","\n","def display_wordcloud(data, width=1200, height=500):\n","    word_draw = WordCloud(\n","        font_path=r\"/Library/Fonts/NanumBarunGothic.ttf\",\n","        width=width, height=height,\n","        stopwords=[\"영화\"], \n","        background_color=\"white\",\n","        random_state=42\n","    )\n","    word_draw.generate(data)\n","\n","    plt.figure(figsize=(15, 7))\n","    plt.imshow(word_draw)\n","    plt.axis(\"off\")\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TWr8Lo2OLD1x","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w2rPptSOLD1z","colab_type":"text"},"source":["## 단어 벡터화 하기\n","### BOW(bag of words)\n","* 가장 간단하지만 효과적이라 널리쓰이는 방법\n","* 장, 문단, 문장, 서식과 같은 입력 텍스트의 구조를 제외하고 각 단어가 이 말뭉치에 얼마나 많이 나타나는지만 헤아립니다.\n","* 구조와 상관없이 단어의 출현횟수만 세기 때문에 텍스트를 담는 가방(bag)으로 생각할 수 있습니다.\n","* BOW는 단어의 순서가 완전히 무시 된다는 단점이 있다. 예를 들어 의미가 완전히 반대인 두 문장이 있다고 합니다.\n","    - `it's bad, not good at all.` \n","    - `it's good, not bad at all.` \n","* 위 두 문장은 의미가 전혀 반대지만 완전히 동일하게 반환됩니다.\n","* 이를 보완하기 위해 n-gram을 사용하는 데 BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용할 수 있도록 합니다.\n","* min_df는 문서에 특정 단어가 최소 몇 번 이상 문서에 등장하는 단어를 가방에 담겠다는 의미입니다.\n","\n","* [Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n","\n","### 사이킷런의 CountVectorizer를 통해 벡터화\n","* 미리 전처리 해둔 텍스트 데이터로 벡터화 합니다.\n","* 모두 소문자로 변환시키기 때문에 영어의 경우 good, Good, gOod이 모두 같은 특성이 됩니다.\n","* 의미없는 특성을 많이 생성하기 때문에 적어도 두 번이상 문서에 나타난 토큰만을 사용한다.\n","* [6.2. Feature extraction — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n","* [sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"]},{"cell_type":"code","metadata":{"id":"m1yYrq66LD10","colab_type":"code","colab":{}},"source":["# analyzer = 'word', # 캐릭터 단위로 벡터화 할 수도 있습니다.\n","# tokenizer = None, # 토크나이저를 따로 지정해 줄 수도 있습니다.\n","# preprocessor = None, # 전처리 도구\n","# stop_words = None, # 불용어 nltk등의 도구를 사용할 수도 있습니다.\n","# min_df = 2, # 토큰이 나타날 최소 문서 개수로 오타나 자주 나오지 않는 특수한 전문용어 제거에 좋다. \n","# ngram_range=(1, 3), # BOW의 단위를 1~3개로 지정합니다.\n","# max_features = 2000 # 만들 피처의 수, 단어의 수가 된다.\n","# CountVectorizer를 불러와서 vectorizer 에 담습니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUWZe5QbLD12","colab_type":"text"},"source":["#### fit, fit_transform의 차이\n","* fit 을 하더라도 fit_transform(raw_documents) 을 호출함"]},{"cell_type":"code","metadata":{"id":"bhLbrr0nLD12","colab_type":"code","colab":{}},"source":["# vectorizer.fit_transform??"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9me8IAVELD14","colab_type":"code","colab":{}},"source":["## fit_transform 으로 train 데이터를 변환합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xVRjGlqLD16","colab_type":"code","colab":{}},"source":["## fit_transform 으로  test 데이터를 변환합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Sn3cmebLD18","colab_type":"code","colab":{}},"source":["# 위에서 vectorizer 결과에서 get_feature_names 를 가져와서 단어 사전을 확인합니다.\n","# 단어사전의 갯수를 세고 미리보기 해봅니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9M8GLWGjLD1-","colab_type":"code","colab":{}},"source":["# np.sum 으로 위에서 구한 train_feature_vector 의 값을 모두 더합니다. axis=0 으로 합니다. \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mfk_mMeNLD2A","colab_type":"code","colab":{}},"source":["# 위에서 구한 빈도수를 그래프로 그립니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJJ7TO1fLD2C","colab_type":"text"},"source":["### TF-IDF\n","TF(단어 빈도, term frequency)는 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값으로, 이 값이 높을수록 문서에서 중요하다고 생각할 수 있다. 하지만 단어 자체가 문서군 내에서 자주 사용되는 경우, 이것은 그 단어가 흔하게 등장한다는 것을 의미한다. 이것을 DF(문서 빈도, document frequency)라고 하며, 이 값의 역수를 IDF(역문서 빈도, inverse document frequency)라고 한다. TF-IDF는 TF와 IDF를 곱한 값이다.\n","\n","IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어 '원자'라는 낱말은 일반적인 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다.\n","\n","역문서 빈도(IDF)는 한 단어가 문서 집합 전체에서 얼마나 공통적으로 나타나는지를 나타내는 값이다. 전체 문서의 수를 해당 단어를 포함한 문서의 수로 나눈 뒤 로그를 취하여 얻을 수 있다.\n","\n","* 출처 : [TF-IDF - 위키백과, 우리 모두의 백과사전](https://ko.wikipedia.org/wiki/TF-IDF)\n","\n","\\begin{equation*}\n","\\text{tfidf}(w, d) = \\text{tf} \\times (\\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1)\n","\\end{equation*}\n","\n","\n","* 싸이킷런 공식문서 : [6.2. Feature extraction — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/feature_extraction.html)\n","\n","#### TfidfTransformer()\n","* norm='l2' 각 문서의 피처 벡터를 어떻게 벡터 정규화 할지 정한다. \n","    - L2 : 벡터의 각 원소의 제곱의 합이 1이 되도록 만드는 것이고 기본 값\n","    - L1 : 벡터의 각 원소의 절댓값의 합이 1이 되도록 크기를 조절\n","* smooth_idf=False\n","    - 피처를 만들 때 0으로 나오는 항목에 대해 작은 값을 더해서(스무딩을 해서) 피처를 만들지 아니면 그냥 생성할지를 결정\n","* sublinear_tf=False\n","* use_idf=True\n","    - TF-IDF를 사용해 피처를 만들 것인지 아니면 단어 빈도 자체를 사용할 것인지 여부"]},{"cell_type":"code","metadata":{"id":"hCduDsUFLD2D","colab_type":"code","colab":{}},"source":["# TfidfTransformer 를 불러와서 transformer 라는 변수에 담습니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M92KCEfFLD2F","colab_type":"code","colab":{}},"source":["# 위에서 벡터화 한 train 데이터를 fit_transform 으로 변환합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"W_ycNTBSLD2H","colab_type":"code","colab":{}},"source":["# 위에서 벡터화 한 test 데이터를 fit_transform 으로 변환합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-i0WyhSbLD2J","colab_type":"code","colab":{}},"source":["#  np.sum 으로 train_feature_tfidf 의 결과 값을 행을 기준으로 더해줍니다.\n","# 사전과 함께 데이터프레임으로 만들고 vocab_count 라는 변수에 저장하고 재사용합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYmyJ2LxLD2M","colab_type":"code","colab":{}},"source":["# 위에서 만든 데이터프레임을 빈도수로 정렬하고 상위 50위 어휘를 막대그래프로 그립니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQRWC5DgLD2O","colab_type":"code","colab":{}},"source":["# label 값을 y_label 에 할당해서 재사용합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p6MV3bT9LD2T","colab_type":"text"},"source":["## 예측하기"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"e5cM8Nq4LD2U","colab_type":"code","colab":{}},"source":["# RandomForestClassifier 를 불러옵니다.\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# 랜덤포레스트 분류기를 사용 합니다. 모델을 선언합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tmedL5U_LD2V","colab_type":"code","colab":{}},"source":["# # cross_val_predict 로 스코어 계산을 해봅니다.\n","# from sklearn.model_selection import cross_val_predict\n","\n","# y_valid = cross_val_predict(model, train_feature_tfidf, y_label, cv=3, n_jobs=-1)\n","# y_valid"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-8JoxxFLD2X","colab_type":"text"},"source":["### 학습과 예측"]},{"cell_type":"code","metadata":{"id":"jqGUJOaCLD2Y","colab_type":"code","colab":{}},"source":["# fit 으로 학습시킵니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2zTlrO1LD2Z","colab_type":"code","colab":{}},"source":["# predict로 예측합니다. \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sfiPapO7LD2b","colab_type":"text"},"source":["### 정답값과 예측값의 갯수 확인"]},{"cell_type":"code","metadata":{"id":"QORM8DU8LD2b","colab_type":"code","colab":{}},"source":["# 예측값의 갯수를 확인합니다.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wL0yj523LD2d","colab_type":"code","colab":{}},"source":["# 정답값을 재사용하기 위해 y_test 라는 변수에 할당합니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQBGFfIpLD2f","colab_type":"text"},"source":["### 점수 보기\n","\n","[sklearn.metrics.f1_score — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"]},{"cell_type":"code","metadata":{"id":"tOyuY2-JLD2g","colab_type":"code","colab":{}},"source":["# accuracy_score, f1_score 를 구해봅니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxuX9dPyLD2i","colab_type":"text"},"source":["### ROC curve 그리기\n","https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html?highlight=roc_curve#sklearn.metrics.plot_roc_curve"]},{"cell_type":"code","metadata":{"id":"NAtdvlC9LD2i","colab_type":"code","colab":{}},"source":["# plot_roc_curve 로 roc_curve 를 그려봅니다.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWMMBdBXLD2o","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}