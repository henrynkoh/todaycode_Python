{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인프런 2020년 새해 다짐 이벤트 댓글 분석\n",
    "* https://www.inflearn.com/pages/newyear-event-20200102\n",
    "* 영상 튜토리얼 : [댓글 수백 수천개 분석하기?! [1/5] 이벤트 데이터 크롤링 feat. 인프런 새해 다짐 이벤트 - YouTube](https://www.youtube.com/watch?v=OUSwQk79H8I&list=PLaTc2c6yEwmohRzCxWQqJ7Z9aqc-sQ5gC)\n",
    "\n",
    "## 기획자나 마케터가 되어 이벤트 댓글을 분석해 보자!\n",
    "### 내가 만약 수백 수천개의 댓글을 다 읽어봐야 한다면?\n",
    "### 댓글 속에 제품에 대한 관심을 빈도수로 추출해야 한다면?\n",
    "* 쇼핑몰에서 제품 관련 이벤트 진행시 어떤 제품을 고객이 선호하는지 알고 싶다면?\n",
    "* 고객 DB와 연계할 수 있다면 이벤트 혹은 마케팅 세그먼트로 활용해 볼 수도 있지 않을까?\n",
    "\n",
    "### 향후 마케팅이나 전략을 어떻게 세워야 할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# 시각화 결과가 선명하게 표시되도록\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화를 위한 한글폰트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window 한글폰트 설정\n",
    "# plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "# Mac 한글폰트 설정\n",
    "plt.rc(\"font\", family=\"AppleGothic\")\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링한 파일 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df 라는 변수에 이벤트 댓글 파일을 로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head 로 미리보기 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail 로 미리보기 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 오류 등으로 발생한 중복 입력 값을 제거\n",
    "* 빈도 수 중복을 방지하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_duplicates 를 통해 중복을 제거합니다. 이때 마지막 글을 남깁니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본은 따로 보존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 전에 원본을 보존하기 위해 origin_text 라는 컬럼에 복사해 둡니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"text\" 파이썬은 대소문자를 구분하기 때문에 데이터 필터링을 위해 대문자를 모두 소문자로 변경\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 같은 의미의 단어를 하나로 통일 예) python => 파이썬\n",
    "# replace 는 텍스트가 완전히 일치될 때만 사용할 수 있습니다.\n",
    "# 일부만 일치한다면 str.replace 를 사용하면 원하는 텍스트로 변경이 가능합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트로 관심 강의 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 이 이벤트에는 \"관심강의\"라는 텍스트가 있습니다.\n",
    "# \"관심강의\"를 기준으로 텍스트를 분리하고 관심강의 뒤에 있는 텍스트를 가져옵니다.\n",
    "# 대부분 \"관심강의\"라는 텍스트를 쓰고 뒤에 강의명을 쓰기 때문입니다.\n",
    "# 전처리한 내용은 실수를 방지하기 위해 \"course\" 라는 새로운 컬럼에 담습니다.\n",
    "# \"관심 강의\", \"관심 강좌\" 에 대해서도 똑같이 전처리 합니다.\n",
    "# \":\" 특수문자를 빈문자로 바꿔줍니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"text\", \"course\" 전처리 내용 미리보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 띄어 쓰기를 제거한 텍스트에서 키워드 추출\n",
    "* TIOBE 프로그래밍 언어 순위 : [index | TIOBE - The Software Quality Company](https://www.tiobe.com/tiobe-index/?fbclid=IwAR34dJfgDHq2DK0C6X3g8IsUno2NhOiikMyxT6fw9SoyujFhy5FPvQogMoA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 키워드가 들어가는 댓글을 찾습니다.\n",
    "search_keyword = ['머신러닝', '딥러닝', '파이썬', '판다스', '공공데이터',\n",
    "                  'django', '크롤링', '시각화', '데이터분석', \n",
    "                  '웹개발', '엑셀', 'c', '자바', '자바스크립트', \n",
    "                  'node', 'vue', '리액트']\n",
    "\n",
    "# for 문을 통해 해당 키워드가 있는지 여부를 True, False값으로 표시하도록 합니다.\n",
    "# 키워드에 따라 컬럼을 새로 만듭니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리보기 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 파이썬|공공데이터|판다스 라는 텍스트가 들어가는 데이터가 있는지 찾습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 결과를 모두 더하면 해당 키워드의 등장 빈도수를 카운트 할 수 있습니다.\n",
    "# search_keyword 컬럼만 가져와서 빈도수를 sum으로 더합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공공데이터 텍스트가 들어가는 문장만 찾습니다.\n",
    "# pandas 를 통해 볼때 문장이 길면 끝까지 보이지 않습니다.\n",
    "# 문장의 전체를 보기 위해 for문을 통해 해당 텍스트를 순회하며 출력합니다.\n",
    "# 이 때, 데이터 사이에 ------ 줄로 구분해서 표시하도록 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 판다스 단어가 들어가는 텍스트만 찾기\n",
    "* 이미 str.contains 를 통해 판다스가 들어가는 텍스트에 대해 컬럼을 만들어 놨습니다. 이 값이  True 라면 판다스 강좌 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pandas 라는 텍스트가 들어가는 내용만 찾습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수 계산을 위한 텍스트 데이터 벡터화\n",
    "* BOW 단어 가방에 단어를 토큰화 해서 담아줌\n",
    "\n",
    "### BOW(bag of words)\n",
    "* 가장 간단하지만 효과적이라 널리쓰이는 방법\n",
    "* 장, 문단, 문장, 서식과 같은 입력 텍스트의 구조를 제외하고 각 단어가 이 말뭉치에 얼마나 많이 나타나는지만 헤아립니다.\n",
    "* 구조와 상관없이 단어의 출현횟수만 세기 때문에 텍스트를 담는 가방(bag)으로 생각할 수 있습니다.\n",
    "* BOW는 단어의 순서가 완전히 무시 된다는 단점이 있다. 예를 들어 의미가 완전히 반대인 두 문장이 있다고 합니다.\n",
    "    - `it's bad, not good at all.` \n",
    "    - `it's good, not bad at all.` \n",
    "* 위 두 문장은 의미가 전혀 반대지만 완전히 동일하게 반환됩니다.\n",
    "* 이를 보완하기 위해 n-gram을 사용하는 데 BOW는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용할 수 있도록 합니다.\n",
    "* min_df는 문서에 특정 단어가 최소 몇 번 이상 문서에 등장하는 단어를 가방에 담겠다는 의미입니다.\n",
    "\n",
    "* [Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "### 사이킷런의 CountVectorizer를 통해 벡터화\n",
    "* 미리 전처리 해둔 텍스트 데이터로 벡터화 합니다.\n",
    "* 모두 소문자로 변환시키기 때문에 영어의 경우 good, Good, gOod이 모두 같은 특성이 됩니다.\n",
    "* 의미없는 특성을 많이 생성하기 때문에 적어도 두 번이상 문서에 나타난 토큰만을 사용한다.\n",
    "* [6.2. Feature extraction — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "* [sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"파이썬 데이터 분석\" 이라는 텍스트를 토큰화 split 을 통해 해봅니다.\n",
    "\"파이썬 데이터 분석\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 CountVectorizer 를 통해 벡터화 합니다.\n",
    "# vectorizer 라는 변수에 CountVectorizer 를 담습니다.\n",
    "# analyzer = 'word', # 캐릭터 단위로 벡터화 할 수도 있습니다.\n",
    "# tokenizer = None, # 토크나이저를 따로 지정해 줄 수도 있습니다.\n",
    "# preprocessor = None, # 전처리 도구\n",
    "# stop_words = None, # 불용어 nltk등의 도구를 사용할 수도 있습니다.\n",
    "# min_df = 2, # 토큰이 나타날 최소 문서 개수로 오타나 자주 나오지 않는 특수한 전문용어 제거에 좋습니다. \n",
    "# ngram_range=(3, 6), # BOW의 단위 갯수의 범위를 지정합니다.\n",
    "# max_features = 2000 # 만들 피처의 수, 단어의 수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['course'] 만 vectorizer로 벡터화 합니다. 결과를 feature_vector 변수에 할당합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer 에서 get_feature_names 를 추출합니다.\n",
    "# vocab 이라는 변수에 할당해서 재사용 합니다.\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 리뷰마다 등장하는 단어에 빈도수가 표현됩니다. 0 은 등장하지 않음을 의미합니다.\n",
    "# feature_vector[:10].toarray() 를 가져와 데이터프레임으로 만들고 컬럼명은 vocab 으로 지정합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 구한 단어벡터를  np.sum으로 더하면 단어가 전체에서 등장하는 횟수를 알 수 있습니다.\n",
    "# 벡터화 된 피처를 확인해 봅니다.\n",
    "# Bag of words 에 몇 개의 단어가 들어있는지 확인합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행과 열의 축을 T로 바꿔주고 빈도수로 정렬합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"course\", \"freq\"] 라는 컬럼명을 주어 위에서 만든 데이터프레임을 변환합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강의명을 토큰 3개로 중복제거하기 위해, 강좌명에서 지식공유자의 이름을 빈문자열로 변경합니다.\n",
    "# 강의명을 lambda 식을 사용해서 x.split() 으로 나누고 [:4] 앞에서 4개까지만 텍스트를 가져오고 다시 join으로 합쳐줍니다. \n",
    "# 중복된 텍스트를 구분해서 보기 위함입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개의 ngram과 빈도수로 역순 정렬을 하게 되면 빈도수가 높고, ngram수가 많은 순으로 정렬이 됨 \n",
    "# 여기에서 drop_duplicates로 첫 번째 강좌를 남기고 나머지 중복을 삭제 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수로 정렬을 하고 어떤 강좌가 댓글에서 가장 많이 언급되었는지 봅니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리가 다 되었다면 다른 팀 또는 담당자에게 전달하기 위해  csv 형태로 저장합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 로 가중치를 주어 벡터화\n",
    "### TfidfTransformer()\n",
    "* norm='l2' 각 문서의 피처 벡터를 어떻게 벡터 정규화 할지 정합니다.\n",
    "    - L2 : 벡터의 각 원소의 제곱의 합이 1이 되도록 만드는 것이고 기본 값(유클리디안거리)\n",
    "    - L1 : 벡터의 각 원소의 절댓값의 합이 1이 되도록 크기를 조절(맨하탄거리)\n",
    "* smooth_idf=False\n",
    "    - 피처를 만들 때 0으로 나오는 항목에 대해 작은 값을 더해서(스무딩을 해서) 피처를 만들지 아니면 그냥 생성할지를 결정\n",
    "* sublinear_tf=False\n",
    "* use_idf=True\n",
    "    - TF-IDF를 사용해 피처를 만들 것인지 아니면 단어 빈도 자체를 사용할 것인지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfTransformer 를 불러와서 가중치를 주어 벡터화 합니다.\n",
    "# transformer 라는 변수로 저장하고 재사용합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform 으로 가중치를 적용하고 결과를 feature_tfidf 로 받습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 row에서 전체 단어가방에 있는 어휘에서 등장하는 단어에 대한 one-hot-vector에 TF-IDF 가중치 반영한 결과를 봅니다.\n",
    "# feature_tfidf.toarray() 로 배열로 만들고  데이터 프레임을 만들어 tfidf_freq 라는 변수에 할당해서 봅니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_freq를 sum 으로 가중치를 다 더해줍니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간에 생략되는 단어를 자세히 보고자 할 때\n",
    "# for문으로 출력해 봅니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 군집화 \n",
    "### KMeans\n",
    "* [2.3. Clustering — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import trange\n",
    "inertia = []\n",
    "\n",
    "start = 30\n",
    "end = 70\n",
    "\n",
    "# 적절한 클러스터의 갯수를 알기 위해 inertia 값을 구함\n",
    "# trange 를 통해 시작과 끝 값을 지정해 주면 진행 정도를 알 수 있습니다.\n",
    "# 학습을 할 때는 feature_tfidf 값을 사용합니다.\n",
    "# cls.inertia_ 값을 inertia 리스트에 저장합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 구한 값을 시각화 합니다.\n",
    "# x축에는 클러스터의 수를 y축에는 inertia 값을 넣어 그립니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 적정한 클러스터 갯수를 넣어 군집화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters 에 적절한 값을 넣어줍니다.\n",
    "# fit.predict 를 하고 결과를 cluster 라는 새로운 컬럼에 담습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"cluster\"] 의 빈도수를 value_counts로 세어봅니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatchKMeans\n",
    "* [Comparison of the K-Means and MiniBatchKMeans clustering algorithms — scikit-learn 0.22.1 documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size 를 쓸 수 있는 MiniBatchKMeans 로 군집화\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "b_inertia = []\n",
    "\n",
    "# 적절한 클러스터의 갯수를 알기 위해 inertia 값을 구함\n",
    "# trange 를 통해 시작과 끝 값을 지정해 주면 진행 정도를 알 수 있습니다.\n",
    "# b_inertia 리스트에 cls.inertia_ 값을 넣어줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 구한 값을 시각화 합니다.\n",
    "# x축에는 클러스터의 수를 y축에는 b_inertia 값을 넣어 그립니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniBatchKMeans 를 통해 학습을 시킵니다.\n",
    "# 결과를 bcluster 라는 변수에 저장합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcluster의 빈도수를 구합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 강좌명이 있는지 특정 클러스터의 값을 봅니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcluster, cluster, course 값을 미리보기 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클러스터 예측 정확도 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters 위에서 정의한 클러스터 수를 사용\n",
    "feature_array = feature_vector.toarray()\n",
    "# 예측한 클러스터의 유니크 값\n",
    "labels = np.unique(prediction)\n",
    "df_cluster_score = []\n",
    "df_cluster = []\n",
    "for label in labels:\n",
    "    id_temp = np.where(prediction==label) # 예측한 값이 클러스터 번호와 매치 되는 것을 가져옴\n",
    "    x_means = np.mean(feature_array[id_temp], axis = 0) # 클러스터의 평균 값을 구함\n",
    "    sorted_means = np.argsort(x_means)[::-1][:n_clusters] # 값을 역순으로 정렬해서 클러스터 수 만큼 가져옴\n",
    "    features = vectorizer.get_feature_names()\n",
    "    best_features = [(features[i], x_means[i]) for i in sorted_means] \n",
    "    # 클러스터별 전체 스코어\n",
    "    df_score = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "    df_cluster_score.append(df_score)\n",
    "    # 클러스터 대표 키워드\n",
    "    df_cluster.append(best_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 클러스터에서 점수가 가장 높은 단어를 추출 아래 점수가 클수록 예측 정확도가 높음\n",
    "# MiniBatchKMeans 로 예측한 값 기준\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score 정확도가 1이 나온 클러스터를 찾아봄 - 같은 강좌끼리 묶였는지 확인 함\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud\n",
    "* [amueller/word_cloud: A little word cloud generator in Python](https://github.com/amueller/word_cloud)\n",
    "* 설치 방법 : [Wordcloud :: Anaconda Cloud](https://anaconda.org/conda-forge/wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 - 자주 등장하지만 의미가 크지 않아 제외하고 싶은 텍스트\n",
    "# stopwords = [\"관심 강의\", \"관심강의\", \"관심\", \"강의\", \"강좌\", \"강의를\",\n",
    "#              \"올해\", \"올해는\", \"열심히\", \"공부를\", \"합니다\", \"하고\", \"싶어요\", \n",
    "#              \"있는\", \"있습니다\", \"싶습니다\", \"2020년\"]\n",
    "# 불용어를 제거하지 않고 그리려면 아래 주석을 풀어서 stopword 변수에 덮어쓰기를 합니다.\n",
    "stopwords = []\n",
    "# displayWordCloud 라는 함수를 만들어 재사용합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
